{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This is an example usage of VI with a PyTorch model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First clone the repo of the model and checkout our customized branch to see\n",
    "the changes required by the VI."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/stamatiad/dino.git\n",
    "%cd dino\n",
    "!git checkout stamatiad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, we have created a wrapper function (save_attn_weights) that\n",
    "wraps the forward method of Attention. Now each time the forward method is\n",
    "called and we have VI recording enabled (with vi.enable_vi() context manager)\n",
    ", we will save the TB summaries in the directory ./vi_logs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git diff main..stamatiad -- vision_transformer.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure you have installed the required python packages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements_colab.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets run our example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Work on the original DINO with PyTorch:\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as pth_transforms\n",
    "from vision_transformer import VisionTransformer\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# VI imports:\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from vit_inspect import vit_inspector as vi\n",
    "from vit_inspect.summary_v2 import vi_summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the pre-trained model and transfer its parameters to a new instance of\n",
    "our modified model, that VI listens to."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the pre-trained model:\n",
    "model_cached = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "# Create a version of the model that holds our attention VI modifications:\n",
    "# Match model params, before load:\n",
    "num_features = model_cached.embed_dim\n",
    "model = VisionTransformer(embed_dim=num_features)\n",
    "model.load_state_dict(model_cached.state_dict(), strict=False)\n",
    "\n",
    "# Enable evaluation mode:\n",
    "device = torch.device(\"cpu\")\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize the VI and inform it about our model parameters:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get some model params, required for VI:\n",
    "vi.params[\"num_layers\"] = len(model.blocks)\n",
    "vi.params[\"num_heads\"] = model.blocks[0].attn.num_heads\n",
    "# The number of tokens when the attention dot product happens.\n",
    "# Here tokens are the patches. Any other feature (e.g. class) is removed.\n",
    "patch_size = model.patch_embed.patch_size\n",
    "crop_size = 480\n",
    "img_size_in_patches = crop_size // patch_size\n",
    "vi.params[\"len_in_patches\"] = img_size_in_patches\n",
    "# Total patches in the image:\n",
    "vi.params[\"num_tokens\"] = img_size_in_patches ** 2\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load a sample image to calculate attention maps uppon."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load sample images:\n",
    "response = requests.get(\"https://dl.fbaipublicfiles.com/dino/img.png\")\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = img.convert('RGB')\n",
    "\n",
    "# Perform the original transformations that the authors did.\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.Resize(img.size),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "img = transform(img)\n",
    "# make the image divisible by the patch size\n",
    "w, h = img.shape[1] - img.shape[1] % patch_size, \\\n",
    "       img.shape[2] - img.shape[2] % patch_size\n",
    "img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "w_featmap = img.shape[-2] // patch_size\n",
    "h_featmap = img.shape[-1] // patch_size\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save a copy of the input image for the VI to display it as preview, making it\n",
    " easier to visualize the attention maps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the input image into the summary:\n",
    "flat_arr_rgb = tf.convert_to_tensor(\n",
    "    # Make sure image's channels is the last dim:\n",
    "    np.moveaxis(np.asarray(img), 1, -1)\n",
    ")\n",
    "with vi.writer.as_default():\n",
    "    step = 0\n",
    "    batch_id = 0\n",
    "    vi.params[\"step\"] = 0\n",
    "    vi.params[\"batch_id\"] = batch_id\n",
    "    vi_summary(\n",
    "        f\"b{batch_id}\",\n",
    "        flat_arr_rgb,\n",
    "        step=step,\n",
    "        description=json.dumps(vi.params)\n",
    "    )\n",
    "    vi.writer.flush()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, perform inference with VI enabled:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the VI context manager to get attention maps of each layer and head:\n",
    "with vi.enable_vi():\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir vi_logs"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
